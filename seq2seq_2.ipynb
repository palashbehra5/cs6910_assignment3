{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import wandb\n",
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import itertools\n",
    "from tqdm import tqdm\n",
    "\n",
    "x_train = np.loadtxt(\"akshar_sequences//x_train.csv\", delimiter=\",\", dtype=int)\n",
    "y_train = np.loadtxt(\"akshar_sequences//y_train.csv\", delimiter=\",\", dtype=int)\n",
    "x_test = np.loadtxt(\"akshar_sequences//x_test.csv\", delimiter=\",\", dtype=int)\n",
    "y_test = np.loadtxt(\"akshar_sequences//y_test.csv\", delimiter=\",\", dtype=int)\n",
    "x_val = np.loadtxt(\"akshar_sequences//x_val.csv\", delimiter=\",\", dtype=int)\n",
    "y_val = np.loadtxt(\"akshar_sequences//y_val.csv\", delimiter=\",\", dtype=int)\n",
    "\n",
    "\n",
    "class SequenceDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        x = torch.from_numpy(self.x[index]).long() \n",
    "        y = torch.from_numpy(self.y[index]).long() \n",
    "        return x, y\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "\n",
    "train_dataset = SequenceDataset(x_train, y_train)\n",
    "val_dataset = SequenceDataset(x_val, y_val)\n",
    "test_dataset = SequenceDataset(x_test, y_test)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "class seq2seq(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers_encoder, num_layers_decoder, \n",
    "                 dropout, bidirectional, encoder_cell_type, decoder_cell_type, teacher_forcing, \n",
    "                 batch_size, max_seq_size, debugging = False):\n",
    "\n",
    "        super(seq2seq, self).__init__()\n",
    "\n",
    "        self.output_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers_encoder = num_layers_encoder\n",
    "        self.num_layers_decoder = num_layers_decoder\n",
    "        self.dropout_prob = dropout\n",
    "        self.bidirectional = bidirectional\n",
    "        self.encoder_cell_type = encoder_cell_type\n",
    "        self.decoder_cell_type = decoder_cell_type\n",
    "        self.teacher_forcing_prob = teacher_forcing\n",
    "        self.debugging = debugging\n",
    "        self.batch_size = batch_size\n",
    "        self.max_seq_size = max_seq_size\n",
    "\n",
    "        self.dropout = nn.Dropout(self.dropout_prob)\n",
    "        self.embedding_encoder = nn.Embedding(num_embeddings=self.output_size, embedding_dim=self.embedding_dim).to(device)\n",
    "        self.embedding_decoder = nn.Embedding(num_embeddings=self.output_size, embedding_dim=self.embedding_dim).to(device)\n",
    "\n",
    "        self.rnn_encoder = self.cell(num_layers_encoder, encoder_cell_type, bool(self.bidirectional))\n",
    "        self.rnn_decoder = self.cell(num_layers_decoder, decoder_cell_type, 0)\n",
    "\n",
    "        # Final layer for calculating probabilities\n",
    "        self.fc1 = nn.Linear(self.hidden_dim, self.output_size)\n",
    "\n",
    "    def cell(self, num_layers, cell_type, bidirectional):\n",
    "\n",
    "        # Defining Cells\n",
    "        cells = {\n",
    "\n",
    "            \"LSTM\" : nn.LSTM(input_size=self.embedding_dim, hidden_size=self.hidden_dim, num_layers=num_layers, batch_first=True, \n",
    "                            dropout=self.dropout_prob, bidirectional=bool(bidirectional)).to(device),\n",
    "\n",
    "            \"GRU\" : nn.GRU(input_size=self.embedding_dim, hidden_size=self.hidden_dim, num_layers=num_layers, batch_first=True, \n",
    "                            dropout=self.dropout_prob, bidirectional=bool(bidirectional)).to(device),\n",
    "\n",
    "            \"RNN\" : nn.RNN(input_size=self.embedding_dim, hidden_size=self.hidden_dim, num_layers=num_layers, batch_first=True, \n",
    "                            dropout=self.dropout_prob, bidirectional=bool(bidirectional)).to(device)\n",
    "\n",
    "        }\n",
    "\n",
    "        return cells[cell_type]\n",
    "    \n",
    "\n",
    "    def initialize_decoder_state(self, encoder_cell_type, encoder_state, encoder_cell, decoder_cell_type, bidirectional,\n",
    "                             num_decoder_layers, num_encoder_layers):\n",
    "\n",
    "        batch_size = encoder_state[0].size(0)  # Get the batch size from the encoder states\n",
    "\n",
    "        if encoder_cell_type == \"LSTM\":\n",
    "            if bidirectional:\n",
    "                forward_state, backward_state = encoder_state[:num_encoder_layers], encoder_state[num_encoder_layers:]\n",
    "                forward_cell, backward_cell = encoder_cell[:num_encoder_layers], encoder_cell[num_encoder_layers:]\n",
    "                encoder_state = (torch.mean(forward_state, dim=0) + torch.mean(backward_state, dim=0)) / 2\n",
    "                encoder_cell = (torch.mean(forward_cell, dim=0) + torch.mean(backward_cell, dim=0)) / 2\n",
    "            else:\n",
    "                encoder_state = torch.mean(encoder_state, dim=0)\n",
    "                encoder_cell = torch.mean(encoder_cell, dim=0) if decoder_cell_type == \"LSTM\" else None\n",
    "        else:\n",
    "            forward_state = encoder_state[:num_encoder_layers]\n",
    "            backward_state = encoder_state[num_encoder_layers:] if bidirectional else None\n",
    "            encoder_state = (torch.mean(forward_state, dim=0) + torch.mean(backward_state, dim=0)) / 2 if bidirectional else torch.mean(forward_state, dim=0)\n",
    "            encoder_cell = None\n",
    "\n",
    "        decoder_state = encoder_state.unsqueeze(0).expand(num_decoder_layers, batch_size, -1)\n",
    "\n",
    "        if decoder_cell_type == \"LSTM\":\n",
    "            decoder_cell = decoder_state\n",
    "        else:\n",
    "            decoder_cell = None\n",
    "\n",
    "        return decoder_state, decoder_cell\n",
    "\n",
    "\n",
    "    # Each forward pass of out network is defined for (batch_size , max_seq_size)\n",
    "    def forward(self, x, y):\n",
    "\n",
    "        x.to(device)\n",
    "        y.to(device)\n",
    "\n",
    "        num_layers_decoder = self.num_layers_decoder\n",
    "        num_layers_encoder = self.num_layers_encoder\n",
    "        batch_size = self.batch_size\n",
    "        output_size = self.output_size\n",
    "        hidden_dim = self.hidden_dim\n",
    "        num_directions = 2 if self.bidirectional else 1\n",
    "        SOS_TOKEN = 128\n",
    "\n",
    "        # Calculate embedding first\n",
    "        # (batch_size , max_sequence_length) -> (batch_size , max_sequence_length, embedding_dimension)\n",
    "        x = self.embedding_encoder(x)\n",
    "\n",
    "        if(self.encoder_cell_type == \"LSTM\") : encoder_output, (encoder_hidden, encoder_cell) = self.rnn_encoder(x)\n",
    "\n",
    "        # hidden_state : (num_directions * num_layers , batch_size , hidden_state_size)\n",
    "        else : \n",
    "            \n",
    "            encoder_output, encoder_hidden = self.rnn_encoder(x)\n",
    "            encoder_cell = None\n",
    "\n",
    "        decoder_state, decoder_cell= self.initialize_decoder_state(self.encoder_cell_type, encoder_hidden, encoder_cell, self.decoder_cell_type, \n",
    "                                                                   self.bidirectional,  num_layers_decoder, num_layers_encoder)\n",
    "        \n",
    "        decoder_inputs = torch.full((batch_size, 1), SOS_TOKEN).to(device)\n",
    "\n",
    "        decoder_outputs = torch.empty((self.max_seq_size, batch_size, self.output_size)).to(device)\n",
    "        \n",
    "        for t in range(self.max_seq_size):\n",
    "            \n",
    "            decoder_inputs = self.embedding_decoder(decoder_inputs.to(device))\n",
    "        \n",
    "            if self.decoder_cell_type == \"LSTM\":\n",
    "                decoder_output, (decoder_state, decoder_cell) = self.rnn_decoder(decoder_inputs, (decoder_state.contiguous(), decoder_cell.contiguous()))\n",
    "            else:\n",
    "                decoder_output, decoder_state = self.rnn_decoder(decoder_inputs, decoder_state.contiguous())\n",
    "                decoder_cell = None\n",
    "\n",
    "            if(num_layers_decoder > 1 ) : decoder_output = self.dropout(decoder_output)\n",
    "\n",
    "            decoder_outputs[t] = self.fc1(decoder_output).squeeze(dim=1)\n",
    "            \n",
    "            # Determine whether to use teacher forcing or predicted output\n",
    "            use_teacher_forcing = random.random() < self.teacher_forcing_prob\n",
    "            \n",
    "            # Obtain the next input to the decoder\n",
    "            if use_teacher_forcing:\n",
    "\n",
    "                decoder_inputs = y[:, t].unsqueeze(0)  # Use ground truth input\n",
    "\n",
    "            else:\n",
    "\n",
    "                #print(decoder_outputs[t].shape)\n",
    "                indices = torch.argmax(decoder_outputs[t], dim=1)\n",
    "\n",
    "                #print(indices.shape)\n",
    "                decoder_inputs = indices.unsqueeze(dim=1)\n",
    "                #print(\"Non Teacher Forcing : \", decoder_inputs.shape)\n",
    "\n",
    "            if (decoder_inputs.shape[0]!= batch_size) : decoder_inputs = decoder_inputs.transpose(0,1)\n",
    "\n",
    "        decoder_outputs = decoder_outputs.transpose(0, 1)\n",
    "\n",
    "        return decoder_outputs\n",
    "    \n",
    "\n",
    "def compare_sequences(batch1, batch2):\n",
    "    \"\"\"\n",
    "    Compare two batches of sequences and return the number of sequences that are exactly the same.\n",
    "\n",
    "    Args:\n",
    "        batch1 (torch.Tensor): Batch of sequences of shape [batch_size, max_seq_length].\n",
    "        batch2 (torch.Tensor): Batch of sequences of shape [batch_size, max_seq_length, vocab_size].\n",
    "\n",
    "    Returns:\n",
    "        int: Number of sequences that are exactly the same.\n",
    "    \"\"\"\n",
    "    # Get the predicted sequences by finding the index of the maximum probability\n",
    "\n",
    "    device = batch1.device\n",
    "    batch2 = batch2.to(device)\n",
    "\n",
    "    predicted_sequences = torch.argmax(batch2, dim=2)\n",
    "\n",
    "    # Compare the predicted sequences with the ground truth sequences\n",
    "    num_same_sequences = torch.sum(torch.all(batch1 == predicted_sequences, dim=1)).item()\n",
    "\n",
    "    return num_same_sequences\n",
    "\n",
    "def test_model_instance(configs):\n",
    "\n",
    "    count = 0\n",
    "\n",
    "    VOCAB_SIZE = 131\n",
    "    BATCH_SIZE = 4\n",
    "    MAX_SEQ_SIZE = 28\n",
    "\n",
    "    source = torch.randint(low=0, high=VOCAB_SIZE, size=(BATCH_SIZE, MAX_SEQ_SIZE)).to(device)\n",
    "    target = torch.randint(low=0, high=VOCAB_SIZE, size=(BATCH_SIZE, MAX_SEQ_SIZE)).to(device)\n",
    "\n",
    "    configs = list(itertools.product(*configs.values()))\n",
    "\n",
    "    for config in tqdm(configs):\n",
    "\n",
    "        input_embedding_size, num_encoder_layers, num_decoder_layers, hidden_layer_size, cell_type_encoder, cell_type_decoder, bidirectional, dropout, teacher_forcing = config\n",
    "    \n",
    "        # Create an instance of the seq2seq model using the parameter values\n",
    "        model = seq2seq(VOCAB_SIZE, input_embedding_size, hidden_layer_size, num_encoder_layers, num_decoder_layers,\n",
    "                   dropout, bidirectional, cell_type_encoder, cell_type_decoder, teacher_forcing,\n",
    "                   BATCH_SIZE, MAX_SEQ_SIZE, debugging=False).to(device)\n",
    "        \n",
    "        output = model(source, target)\n",
    "\n",
    "        if(output.shape[0]==BATCH_SIZE and output.shape[1]==MAX_SEQ_SIZE and output.shape[2]==VOCAB_SIZE) : count+=1\n",
    "        \n",
    "    print(\"PASSED {} CONFIGS.\".format(count))\n",
    "    \n",
    "\n",
    "\n",
    "wandb.login()\n",
    "\n",
    "sweep_config = {\n",
    "    \"method\": \"bayes\",\n",
    "    \"metric\":{\n",
    "    \"name\": \"val_accuracy\",\n",
    "    \"goal\": \"maximize\"\n",
    "    },\n",
    "    'parameters': {\n",
    "        'input_embedding_size': {'values': [128, 256, 512, 1024]},\n",
    "        'num_encoder_layers': {'values': [3, 4, 5, 6]},\n",
    "        'num_decoder_layers': {'values': [3, 4, 5, 6]},\n",
    "        'hidden_layer_size' : {'values' : [32, 64]},\n",
    "        'cell_type_encoder' : {'values' : ['LSTM', 'GRU', 'RNN']},\n",
    "        'cell_type_decoder' : {'values' : ['LSTM', 'GRU', 'RNN']},\n",
    "        'bidirectional' : {'values' : [0, 1]},\n",
    "        'dropout' : {'values' : [0,0.2,0.3]},\n",
    "        'teacher_forcing' : {'values' : [0, 0.5, 0.75, 1]},\n",
    "        'batch_size' : {'values' : [8,16,64,128,256]}\n",
    "    }\n",
    "}\n",
    "\n",
    "sweep_id = wandb.sweep(sweep_config, project=\"seq2seq_runs_2\")\n",
    "\n",
    "def train(config=None):\n",
    "   \n",
    "    wandb.init(config=config)\n",
    "    run_name = \"ies_\"+str(wandb.config.input_embedding_size)+\"_nel_\"+str(wandb.config.num_encoder_layers)+\"_ndl_\"+str(wandb.config.num_decoder_layers)+\"_hls_\"+str(wandb.config.hidden_layer_size)+\"_cte_\"+str(wandb.config.cell_type_encoder)+\"_ctd_\"+str(wandb.config.cell_type_decoder)+\"_tf_\"+str(wandb.config.teacher_forcing)+\"_bs_\"+str(wandb.config.batch_size)\n",
    "    wandb.run.name = run_name\n",
    "    config = wandb.config\n",
    "\n",
    "    VOCAB_SIZE = 131\n",
    "    EMBEDDING_DIM = config.input_embedding_size\n",
    "    HIDDEN_DIM = config.hidden_layer_size\n",
    "    EPOCHS = 25\n",
    "    NUM_LAYERS_ENCODER = config.num_encoder_layers\n",
    "    NUM_LAYERS_DECODER =config.num_decoder_layers\n",
    "    DROPOUT = config.dropout\n",
    "    BIDIRECTIONAL = config.bidirectional\n",
    "    CELL_TYPE_ENCODER = config.cell_type_encoder\n",
    "    CELL_TYPE_DECODER = config.cell_type_decoder\n",
    "    TEACHER_FORCING = config.teacher_forcing\n",
    "    MAX_SEQ_SIZE = 28\n",
    "    BATCH_SIZE = config.batch_size\n",
    "    \n",
    "    train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    model = seq2seq(VOCAB_SIZE, EMBEDDING_DIM, HIDDEN_DIM, NUM_LAYERS_ENCODER, NUM_LAYERS_DECODER, \n",
    "                    DROPOUT, BIDIRECTIONAL, CELL_TYPE_ENCODER, CELL_TYPE_DECODER, TEACHER_FORCING, \n",
    "                    BATCH_SIZE, MAX_SEQ_SIZE, debugging = False)\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        \n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        train_accuracy = 0\n",
    "        val_accuracy = 0\n",
    "        \n",
    "        for batch_idx, (inputs, targets) in (enumerate(train_loader)):\n",
    "            \n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs, targets)\n",
    "            \n",
    "            train_accuracy += compare_sequences(targets, outputs)\n",
    "\n",
    "            loss = criterion(outputs.reshape(-1, model.output_size), targets.reshape(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "        \n",
    "\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in (val_loader):\n",
    "            \n",
    "                inputs = inputs.to(device)\n",
    "                targets = targets.to(device)\n",
    "                outputs = model(inputs, targets)\n",
    "\n",
    "                loss = criterion(outputs.reshape(-1, model.output_size), targets.reshape(-1))\n",
    "                val_accuracy += compare_sequences(targets, outputs)\n",
    "                \n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        wandb.log({\"train_loss\": running_loss/len(train_loader), \"val_loss\": val_loss/len(val_loader), \"epochs\" : epoch, \n",
    "                   \"training_accuracy\" : train_accuracy/len(train_dataset), \"val_accuracy\" : val_accuracy/len(val_dataset)})       \n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "#3d1f06c5fc41d6f98a95bd18ccf2a65afc46a1b5\n",
    "wandb.agent(sweep_id, train, count = 100) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
